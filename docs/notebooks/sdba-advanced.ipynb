{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Downscaling and Bias-Adjustment - Advanced tools\n",
    "\n",
    "The previous notebook covered the most common utilities of `xclim.sdba` for conventionnal cases. Here we explore more advanced usage of `xclim.sdba` tools.\n",
    "\n",
    "## LOESS smoothing and detrending\n",
    "\n",
    "As described in Cleveland (1979), locally weighted linear regressions are multiple regression methods using a nearest-neighbor approach. Instead of using all data points to compute a linear or polynomial regression, LOESS algorithms compute a local regression for each point in the dataset, using only the k-nearest neighbors as selected by a weighting function. This weighting function must fulfill some strict requirements, see the doc of `xclim.sdba.loess.loess_smoothing` for more details.\n",
    "\n",
    "In xclim's implementation, the user can choose between local _constancy_ ($d=0$, local estimates are weighted averages) and local _linearity_ ($d=1$, local estimates are taken from linear regressions). Two weighting functions are currently implemented : \"tricube\" ($w(x) = (1 - x^3)^3$) and \"gaussian\" ($w(x) = e^{-x^2 / 2\\sigma^2}$). Finally, the number of Cleveland's _robustifying iterations_ is controllable through `niter`. After computing an estimate of $y(x)$, the weights are modulated by a function of the distance between the estimate and the points and the procedure is started over. These iterations are made to weaken the effect of outliers on the estimate.\n",
    "\n",
    "The next example shows the application of the LOESS to daily temperature data. The black line and dot are the estimated $y$, outputs of the `sdba.loess.loess_smoothing` function, using local linear regression (passing $d = 1$), a window spanning 20% ($f = 0.2$) of the domain, the \"tricube\" weighting function and only one iteration. The red curve illustrates the weighting function on January 1st 2014, where the red circles are the nearest-neighbors used in the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xclim.sdba import loess\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily temperature data from xarray's tutorials\n",
    "ds = xr.tutorial.open_dataset('air_temperature').resample(time='D').mean()\n",
    "tas = ds.isel(lat=0, lon=0).air\n",
    "\n",
    "# Compute the smoothed series\n",
    "f = 0.2\n",
    "ys = loess.loess_smoothing(tas, d=1, weights='tricube', f=f, niter=1)\n",
    "\n",
    "# Plot data points and smoothed series\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tas.time, tas, 'o', fillstyle='none')\n",
    "ax.plot(tas.time, ys, 'k')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Temperature [K]')\n",
    "\n",
    "## The code below calls internal functions to demonstrate how the weights are computed. \n",
    "\n",
    "# LOESS algorithms as implemented here use scaled coordinates.\n",
    "x = tas.time\n",
    "x = (x - x[0]) / (x[-1] - x[0])\n",
    "xi = x[366]\n",
    "ti = tas.time[366]\n",
    "\n",
    "# Weighting function take the distance with all neighbors scaled by the r parameter as input\n",
    "r = int(f * tas.time.size)\n",
    "h = np.sort(np.abs(x - xi))[r]\n",
    "weights = loess._tricube_weighting(np.abs(x - xi).values / h)\n",
    "\n",
    "# Plot nearest neighbors and weighing function\n",
    "wax = ax.twinx()\n",
    "wax.plot(tas.time, weights, color='indianred')\n",
    "ax.plot(tas.time, tas.where(tas * weights > 0), 'o', color='lightcoral', fillstyle='none')\n",
    "\n",
    "ax.plot(ti, ys[366], 'ko')\n",
    "wax.set_ylabel('Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOESS smoothing can suffer from heavy boundary effects. On the previous graph, we can associate the strange bend on the left end of the line to them. The next example shows a stronger case. Usually, $\\frac{f}{2}N$ points on each side should be discarded. On the other hand, LOESS has the advantage of always staying within the bounds of the data.\n",
    "\n",
    "\n",
    "### LOESS Detrending\n",
    "\n",
    "In climate science, it can be used in the detrending process. `xclim` provides `sdba.detrending.LoessDetrend` in order to compute trend with the LOESS smoothing and remove them from timeseries.\n",
    "\n",
    "First we create some toy data with a sinusoidal annual cycle, random noise and a linear temperature increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = xr.cftime_range('1990-01-01', '2049-12-31', calendar='noleap')\n",
    "tas = xr.DataArray(\n",
    "   (10 * np.sin(time.dayofyear * 2 * np.pi / 365) +  # Annual variability\n",
    "    5 * (np.random.random_sample(time.size) - 0.5) +  # Random noise\n",
    "    np.linspace(0, 1.5, num=time.size)),  # 1.5 degC increase in 60 years\n",
    "    dims=('time',), coords={'time': time},\n",
    "    attrs={'units': 'degC'}, name='temperature',\n",
    ")\n",
    "tas.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the trend on the data. Here, we compute on the whole timeseries (`group='time'`) with the parameters suggested above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclim.sdba.detrending import LoessDetrend\n",
    "\n",
    "# Create the detrending object\n",
    "det = LoessDetrend(group='time', d=0, niter=2, f=0.2)\n",
    "# Fitting returns a new object\n",
    "fit = det.fit(tas)\n",
    "# Get the trend and the detrended series\n",
    "trend = fit.get_trend(tas)\n",
    "tas_det = fit.detrend(tas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "trend.plot(ax=ax, label='Computed trend')\n",
    "ax.plot(time, np.linspace(0, 1.5, num=time.size), label='Expected tred')\n",
    "ax.plot([time[0], time[int(0.1 * time.size)]], [0.4, 0.4], linewidth=6, color=\"gray\")\n",
    "ax.plot([time[-int(0.1 * time.size)], time[-1]], [1.1, 1.1], linewidth=6, color=\"gray\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said earlier, this example shows how the Loess has strong boundary effects. It is recommended to remove the $\\frac{f}{2}\\cdot N$ outermost points on each side, as shown by the  gray bars in the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing an Adjustment object from a training dataset\n",
    "\n",
    "For large scale uses, when the training step deserves its own computation and write to disk, or simply when there are multiples `sim` to be adjusted with the same training, it is helpful to be able to instantiate the Adjustment objects from the training dataset itself. This trick relies on a global attribute \"adj_params\" set on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Create toy data for the example, here fake temperature timeseries\n",
    "t = xr.cftime_range('2000-01-01', '2030-12-31', freq='D', calendar='noleap')\n",
    "ref = xr.DataArray((-20 * np.cos(2 * np.pi * t.dayofyear / 365) + 2 * np.random.random_sample((t.size,)) + 273.15\n",
    "                    + 0.1 * (t - t[0]).days / 365),  # \"warming\" of 1K per decade,\n",
    "                   dims=('time',), coords={'time': t}, attrs={'units': 'K'})\n",
    "sim = xr.DataArray((-18 * np.cos(2 * np.pi * t.dayofyear / 365) + 2 * np.random.random_sample((t.size,)) + 273.15\n",
    "                    + 0.11 * (t - t[0]).days / 365),  # \"warming\" of 1.1K per decade\n",
    "                   dims=('time',), coords={'time': t}, attrs={'units': 'K'})\n",
    "\n",
    "ref = ref.sel(time=slice(None, '2015-01-01'))\n",
    "hist = sim.sel(time=slice(None, '2015-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclim.sdba.adjustment import QuantileDeltaMapping\n",
    "\n",
    "QDM = QuantileDeltaMapping(nquantiles=15, kind='+', group='time.dayofyear')\n",
    "QDM.train(ref, hist)\n",
    "QDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained `QDM` exposes the training data in the `ds` attribute, Here, we will write it to disk, read it back and initialize an new object from it. Notice the `adj_params` in the dataset, that has the same value as the repr string printed just above. Also, notice the `_xclim_adjustment` attribute that contains a json string so we can rebuild the adjustment object later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDM.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDM.ds.to_netcdf('QDM_training.nc')\n",
    "ds = xr.open_dataset('QDM_training.nc')\n",
    "QDM2 = QuantileDeltaMapping.from_dataset(ds)\n",
    "QDM2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case above, creating a full object from the dataset doesn't make the most sense since we are in the same python session, with the \"old\" object still available. This method effective when we reload the training data in a different python session, say on another computer. **However, take note that there is no retrocompatiblity insurance.** If the QuantileDeltaMapping class was to change in a new xclim version, one would not be able to create the new object from a dataset saved with the old one.\n",
    "\n",
    "For the case where we stay in the same python session, it is still useful to trigger the dask computations. For small datasets, that could mean a simple `QDM.ds.load()`, but sometimes even the training data is too large to be full loaded in memory. In that case, we could also do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDM.ds.to_netcdf('QDM_training2.nc')\n",
    "ds = xr.open_dataset('QDM_training2.nc')\n",
    "ds.attrs['title'] = 'This is the dataset, but read from disk.'\n",
    "QDM.set_dataset(ds)\n",
    "QDM.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDM2.adjust(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
